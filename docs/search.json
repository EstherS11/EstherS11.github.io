[
  {
    "objectID": "tsinghua.html",
    "href": "tsinghua.html",
    "title": "Esther Sun",
    "section": "",
    "text": "üìç With the IIIS Department Football Team\n\n\n\n\n\n\n\n\n\n\n\nüìç Photo with Turing Award winner Andrew Yao\n\n\n\n\n\nüìç Host of the Tsinghua Ma John Cup Football Final\n\n\n\n\n\nüìç Host of the Tsinghua Ma John Cup Football Final\n\n\n\n\n\nüìç Conference Photographer for MECC 2025\n\n\n\n\n\n\n\n\nüìç Soccer Referee in Tsinghua\n\n\n\n\n\nüìç Soccer Referee in Tsinghua\n\n\n\n\n\n\n\n\nüìç Poster presentation in ACM MM 2023 Owatta\n\n\n\n\n\nüìç Oral presentation in ACM MM 2023 Owatta\n\n\n\n\n\nüìç Oral presentation in ACM MM 2023 Owatta\n\n\n\n\n\nüìç ICML 2025, Vancouver\n\n\n\n\n\nüìç ICML 2025, Vancouver\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüìç Student representative for ACM Canadian Celebration of Women in Computing Conference, Toronto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Home",
    "section": "About Me",
    "text": "About Me\n\nI am a Master‚Äôs student at Carnegie Mellon University (Class of 2026), where I focus on Multimodal Machine Learning. I am privileged to be supervised by Prof.¬†Carlos Busso. My academic journey began at the University of Toronto, where I earned an Honors Bachelor of Science with a double major in Computer Science and Statistical Science. During my undergraduate years, I also spent time as an exchange student at Tsinghua University‚Äôs Institute for Interdisciplinary Information Sciences (IIIS/Yao Class).\nMy research trajectory has evolved from early work in multimodal CV‚ÄìNLP systems ‚Äî particularly explainable image forgery detection and multimodal fact verification ‚Äî toward a current specialization in Affective Computing and Emotion Intelligence, focusing on multimodal emotion recognition and AI persona. My recent work centers on understanding emotional ambiguity in real-world human communication, such as ADEPT (Agentic Decoding of Emotion via Probing Tools), which combine multimodal large language models with reinforcement learning (GRPO) to generate evidence-grounded rationales for ambiguous emotional states. I believe the future of AI lies not just in providing the ‚Äúright‚Äù answer, but in communicating with emotional intelligence and context-aware empathy‚Äîensuring that agents respond in the most appropriate and human-centric manner."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Home",
    "section": "Education",
    "text": "Education\n\n\n\n\nCarnegie Mellon University\nMaster Student | üìç Pittsburgh, USA\nFocus: Multimodal Machine Learning\n\n\n\n\n\nUniversity of Toronto\nHonors B.Sc. | üìç Toronto, Canada\nDouble Major: Computer Science & Statistical Science\n\n\n\n\n\nTsinghua University\nExchange Student | IIIS (Yao Class) | üìç Beijing, China"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Home",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n\nICML 2026 Under Review\n\nADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools ‚Äî From Consensus Learning to Ambiguity-Driven Emotion Reasoning\nFirst Author\nAgentic LLM reasoning ¬∑ Tool-augmented inference ¬∑ RL alignment (GRPO)\n\n\n\n\n\n\nICASSP 2026 Accepted\n\nRecovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration\nFirst Author\nDiscrete Audio Tokenization ¬∑ Multi-layer Attention Fusion ¬∑ SSL ¬∑ Neural Codecs\n\n\n\n\n\n\nNeurIPS 2025 Workshop Accepted\n\nSystematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications\nFirst Author | üîó Paper\nAI Companionship ¬∑ Embodied Intelligence ¬∑ Technical Taxonomy\n\n\n\n\n\n\nIEEE TPAMI 2024 Under Review\n\nForgeryGPT: Multimodal Large Language Model for Explainable Image Forgery Detection and Localization\nCo-author\nFine-grained forgery localization ¬∑ Vision‚Äìlanguage reasoning ¬∑ Multimodal LLM grounding\n\n\n\n\n\n\nACM MM 2023 Accepted\n\nECENet: Explainable and Context-Enhanced Network for Multimodal Fact Verification\nCo-author\nDual-granularity Attention ¬∑ Cross-modal Alignment ¬∑ Hierarchical Reasoning\n\n\n\n\n\n\nAAAI 2023 Accepted\n\nUnimodal Feature-Enhanced and Cross-Modal Correlation Learning for Multimodal Fact Verification\nCo-author\nMultimodal feature engineering ¬∑ Cross-modal correlation learning"
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "Home",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n\nData Scientist Intern ‚Äî Moneris\nüìç Toronto, Canada ¬∑ May 2022 ‚Äì Sep 2023 (16-month)\nMachine Learning Systems ¬∑ Imbalanced Learning ¬∑ Feature Engineering ¬∑ Production ML Pipelines\n\n\n\n\n\nArt Studio Assistant\nSummer 2019\nIllustration & Visual Storytelling ¬∑ Visual Design ¬∑ Color Composition"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Interspeech 2026 Working on\n\nLearning from Minority Perception: Improving Explainable SER via Rationale Fine-tuning\nFirst Author\nHierarchical Emotion Modeling ¬∑ Cross-Model Distillation ¬∑ Explainable SER\n\nKey Points: TBD\n\n\n\n\n\n\n\n\n\n\nICML 2026 Under Review\n\nADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools ‚Äî From Consensus Learning to Ambiguity-Driven Emotion Reasoning\nFirst Author\nAgentic LLM reasoning ¬∑ Tool-augmented inference ¬∑ RL alignment (GRPO)\n\nTL;DR: Proposes ADEPT, a pioneering agentic framework that transforms Speech Emotion Recognition (SER) from a static classification task into an active, evidence-grounded reasoning process using RL-aligned MLLMs. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning.\n\nKey Points:\n\nAutonomous Agentic Framework: Engineered an MLLM-based autonomous agent designed for multi-turn Speech Emotion Recognition (SER). The framework redefines emotion inference as an iterative inquiry process, where the agent programmatically orchestrates a specialized toolkit of semantic and acoustic probes to decode complex paralinguistic cues.\nEvidence-Grounded Rationalization: Developed a retrieval-augmented rationalization mechanism that anchors model predictions in verifiable physical evidence. By extracting and analyzing pitch trajectories, energy profiles, and spectral centroids, the system effectively mitigates text-biased hallucinations and provides high-fidelity, auditable reasoning traces.\nReinforcement Learning Alignment (GRPO): Implemented Group Relative Policy Optimization (GRPO) to refine the agent‚Äôs multi-step decision-making trajectories. This alignment strategy optimizes the policy-driven evidence acquisition process, rewarding logical rigor and penalizing non-informative tool invocations to ensure the reasoning path aligns with human-expert diagnostic standards.\nAmbiguity-Driven Reasoning: Formulated a novel approach to address the ‚ÄúConsensus Paradox‚Äù by treating annotator disagreement as a valuable supervision signal rather than noise. By modeling emotional ambiguity through a consensus-driven inquiry process, the framework significantly improves robustness and recovery of co-occurring minor emotions on benchmark datasets including MSP-Podcast and IEMOCAP.\n\nThis work explores how agentic multimodal models can move beyond pattern recognition toward verifiable, evidence-grounded reasoning under perceptual ambiguity.\n\n\n\n\n\n\n\n\nICASSP 2026 Accepted\n\nRecovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration\nFirst Author\nDiscrete Audio Tokenization ¬∑ Multi-layer Attention Fusion ¬∑ SSL ¬∑ Neural Codecs\n\nTL;DR: Introduces a multi-layer fusion framework to recover the significant performance loss in Speech Emotion Recognition (SER) caused by audio discretization, enabling semantic-rich discrete tokens to rival high-fidelity continuous features.\n\nKey Points:\n\nPerformance Recovery: Developed a novel framework that recovers 75% of the performance drop induced by discretization by integrating multi-layer WavLM tokens with 74-dimensional openSMILE paralinguistic features.\nArchitecture Innovation: Designed and benchmarked dual fusion strategies‚ÄîLayer-First and Modality-First‚Äîachieving an 8% gain with the Layer-First approach by prioritizing hierarchical feature extraction.\nAblation & Trade-offs: Conducted systematic studies across 24 WavLM layers and 5 codebook sizes (K=256 to 4000) to identify optimal compression-accuracy trade-offs for emotion preservation.\nNeural Codec Benchmarking: Evaluated against three major neural codecs (SpeechTokenizer, DAC, and EnCodec), demonstrating that semantic-rich SSL tokens provide a 50%+ performance advantage over reconstruction-focused models for downstream SER tasks.\n\nThis work establishes semantic-rich discrete speech tokens as a viable alternative to continuous acoustic features for affective computing, bridging representation learning and downstream emotion reasoning under aggressive compression constraints.\n\n\n\n\n\n\n\n\n\n\n\nNeurIPS 2025 Workshop Accepted\n\nSystematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications\nFirst Author | üîó Paper\nAI Companionship ¬∑ Embodied Intelligence ¬∑ Technical Taxonomy\n\nTL;DR: Systematizes the fragmented landscape of AI persona design by introducing a novel four-quadrant technical taxonomy that maps companion applications from virtual agents to embodied systems, providing actionable design guidelines for researchers and practitioners.\n\nKey Points:\n\nFour-Quadrant Taxonomy: Proposed a systematic framework categorizing AI companions along two orthogonal axes‚Äîembodiment level and interaction modality‚Äîto clarify design trade-offs across diverse application domains.\nTechnical Design Patterns: Identified and documented recurring architectural patterns for persona consistency, memory management, and emotional coherence across 50+ commercial and research systems.\nEmbodiment Spectrum Analysis: Analyzed the progression from text-only virtual companions to fully embodied robotic agents, mapping key technical challenges at each stage of the embodiment continuum.\nActionable Guidelines: Synthesized practical design recommendations for balancing persona authenticity, user safety, and system scalability in real-world AI companion deployments.\n\nThis work provides a unifying systems-level lens for LLM persona agents, enabling principled design and evaluation of AI companions across virtual, multimodal, and embodied interaction settings.\n\n\n\n\n\n\n\n\n\n\n\nIEEE TPAMI 2024 Under Review\n\nForgeryGPT: Multimodal Large Language Model for Explainable Image Forgery Detection and Localization\nCo-author\nFine-grained forgery localization ¬∑ Vision‚Äìlanguage reasoning ¬∑ Multimodal LLM grounding\n\nTL;DR: Leverages Multimodal LLMs to pioneer an explainable image forgery detection and localization system that generates natural language rationales for identified visual manipulations.\n\nKey Points:\n\nHybrid Vision Architecture: Developed a ‚ÄúVocabulary-enhanced Vision Encoder‚Äù integrating a trainable ViT with a frozen CLIP encoder to capture fine-grained, domain-specific forgery artifacts.\nAdaptive Forgery Prompting: Proposed an ‚ÄúObject-agnostic Forgery Prompt‚Äù mechanism using 12-dimensional learnable embeddings to enable dynamic adaptation across diverse forgery scenarios, achieving a 15% improvement over baselines.\nLarge-Scale Synthesis Pipeline: Constructed a comprehensive 768K+ multimodal dataset using a multi-granularity mask generation pipeline that combines random segmentation with Segment Anything Model (SAM) techniques.\nExplainable Localization: Contributed the FL-Expert module to provide precise spatial localization coupled with linguistic explanations of splicing, copy-move, and removal manipulations.\n\n\n\n\n\n\n\n\n\n\n\n\nACM MM 2023 Accepted\n\nECENet: Explainable and Context-Enhanced Network for Multimodal Fact Verification\nCo-author\nDual-granularity Attention ¬∑ Cross-modal Alignment ¬∑ Hierarchical Reasoning\n\nTL;DR: Introduces a state-of-the-art multimodal fact verification framework that utilizes dual-granularity attention and hierarchical reasoning to generate evidence-based justifications for news veracity.\n\nKey Points:\n\nDual-Granularity Attention: Engineered an Improved Coarse- and Fine-grained Attention Network (CFgAN) that enhanced contextual comprehension of image-text correlations by 12.1%.\nHierarchical Reasoning Framework: Developed a unified architecture for feature extraction and cross-modal fusion, enabling the system to perform complex evidence-based inference.\nBenchmark Leadership: Achieved SOTA performance on major benchmarks, including 87.7% accuracy on NewsCLIPpings and an 81.5 F1-score on FACTIFY.\nJustification Generation: Focused on ‚Äúexplainability‚Äù by ensuring the network provides logical justifications alongside its verification results, as showcased in oral and poster presentations at ACM MM 2023.\n\n\n\n\n\n\n\n\n\nAAAI 2023 Accepted\n\nUnimodal Feature-Enhanced and Cross-Modal Correlation Learning for Multimodal Fact Verification\nCo-author\nMultimodal feature engineering ¬∑ Cross-modal correlation learning"
  },
  {
    "objectID": "artwork.html#dinosaur",
    "href": "artwork.html#dinosaur",
    "title": "Artwork",
    "section": "Dinosaur",
    "text": "Dinosaur"
  },
  {
    "objectID": "artwork.html#random",
    "href": "artwork.html#random",
    "title": "Artwork",
    "section": "Random",
    "text": "Random"
  }
]