[
  {
    "objectID": "tsinghua.html",
    "href": "tsinghua.html",
    "title": "Esther Sun",
    "section": "",
    "text": "ğŸ“ With the IIIS Department Football Team\n\n\n\n\n\n\n\n\n\n\n\nğŸ“ Photo with Turing Award winner Andrew Yao\n\n\n\n\n\nğŸ“ Host of the Tsinghua Ma John Cup Football Final\n\n\n\n\n\nğŸ“ Host of the Tsinghua Ma John Cup Football Final\n\n\n\n\n\nğŸ“ Conference Photographer for MECC 2025\n\n\n\n\n\n\n\n\nğŸ“ Soccer Referee in Tsinghua\n\n\n\n\n\nğŸ“ Soccer Referee in Tsinghua\n\n\n\n\n\n\n\n\nğŸ“ Poster presentation in ACM MM 2023 Owatta\n\n\n\n\n\nğŸ“ Oral presentation in ACM MM 2023 Owatta\n\n\n\n\n\nğŸ“ Oral presentation in ACM MM 2023 Owatta\n\n\n\n\n\nğŸ“ ICML 2025, Vancouver\n\n\n\n\n\nğŸ“ ICML 2025, Vancouver\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ“ Student representative for ACM Canadian Celebration of Women in Computing Conference, Toronto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Home",
    "section": "About Me",
    "text": "About Me\n\nI am a Masterâ€™s student at Carnegie Mellon University (Class of 2026), where I focus on Multimodal Machine Learning. I am privileged to be supervised by Prof.Â Carlos Busso. My academic journey began at the University of Toronto, where I earned an Honors Bachelor of Science with a double major in Computer Science and Statistical Science. During my undergraduate years, I also spent time as an exchange student at Tsinghua Universityâ€™s Institute for Interdisciplinary Information Sciences (IIIS/Yao Class).\nMy research trajectory has evolved from early work in multimodal CVâ€“NLP systems â€” particularly explainable image forgery detection and multimodal fact verification â€” toward a current specialization in Affective Computing and Emotion Intelligence, focusing on multimodal emotion recognition and AI persona. My recent work centers on understanding emotional ambiguity in real-world human communication, such as ADEPT (Agentic Decoding of Emotion via Probing Tools), which combines multimodal large language models with reinforcement learning (GRPO) to generate evidence-grounded rationales for ambiguous emotional states. I believe the future of AI lies not just in providing the â€œrightâ€ answer, but in communicating with emotional intelligence and context-aware empathyâ€”ensuring that agents respond in the most appropriate and human-centric manner."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Home",
    "section": "Education",
    "text": "Education\n\n\n\n\nCarnegie Mellon University\nMaster Student | ğŸ“ Pittsburgh, USA\nFocus: Multimodal Machine Learning\n\n\n\n\n\nUniversity of Toronto\nHonors B.Sc. | ğŸ“ Toronto, Canada\nDouble Major: Computer Science & Statistical Science\n\n\n\n\n\nTsinghua University\nExchange Student | IIIS (Yao Class) | ğŸ“ Beijing, China"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Home",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n\nICML 2026 Under Review\n\nADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools â€” From Consensus Learning to Ambiguity-Driven Emotion Reasoning\nFirst Author\nAgentic LLM reasoning Â· Tool-augmented inference Â· RL alignment (GRPO)\n\n\n\n\n\n\nICASSP 2026 Accepted\n\nRecovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration\nFirst Author | ğŸ”— Paper\nDiscrete Audio Tokenization Â· Multi-layer Attention Fusion Â· SSL Â· Neural Codecs\n\n\n\n\n\n\nNeurIPS 2025 Workshop Accepted\n\nSystematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications\nFirst Author | ğŸ”— Paper\nAI Companionship Â· Embodied Intelligence Â· Technical Taxonomy\n\n\n\n\n\n\nIEEE TPAMI 2024 Under Review\n\nForgeryGPT: Multimodal Large Language Model for Explainable Image Forgery Detection and Localization\nCo-author | ğŸ”— Paper\nFine-grained forgery localization Â· Visionâ€“language reasoning Â· Multimodal LLM grounding\n\n\n\n\n\n\nACM MM 2023 Accepted\n\nECENet: Explainable and Context-Enhanced Network for Multimodal Fact Verification\nCo-author | ğŸ”— Paper\nDual-granularity Attention Â· Cross-modal Alignment Â· Hierarchical Reasoning\n\n\n\n\n\n\nAAAI 2023 Accepted\n\nUnimodal Feature-Enhanced and Cross-Modal Correlation Learning for Multimodal Fact Verification\nCo-author | ğŸ”— Paper\nMultimodal feature engineering Â· Cross-modal correlation learning"
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "Home",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n\nData Scientist Intern â€” Moneris\nğŸ“ Toronto, Canada Â· May 2022 â€“ Sep 2023 (16-month)\nMachine Learning Systems Â· Imbalanced Learning Â· Feature Engineering Â· Production ML Pipelines\n\n\n\n\n\nArt Studio Assistant\nSummer 2019\nIllustration & Visual Storytelling Â· Visual Design Â· Color Composition"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Interspeech 2026 Working on\n\nLearning from Minority Perception: Improving Explainable SER via Rationale Fine-tuning\nFirst Author\nHierarchical Emotion Modeling Â· Cross-Model Distillation Â· Explainable SER\n\nKey Points: TBD\n\n\n\n\n\n\n\n\n\n\nICML 2026 Under Review\n\nADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools â€” From Consensus Learning to Ambiguity-Driven Emotion Reasoning\nFirst Author\nAgentic LLM reasoning Â· Tool-augmented inference Â· RL alignment (GRPO)\n\nTL;DR: Proposes ADEPT, a pioneering agentic framework that transforms Speech Emotion Recognition (SER) from a static classification task into an active, evidence-grounded reasoning process using RL-aligned MLLMs. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning.\n\nKey Points:\n\nAutonomous Agentic Framework: Engineered an MLLM-based autonomous agent designed for multi-turn Speech Emotion Recognition (SER). The framework redefines emotion inference as an iterative inquiry process, where the agent programmatically orchestrates a specialized toolkit of semantic and acoustic probes to decode complex paralinguistic cues.\nEvidence-Grounded Rationalization: Developed a retrieval-augmented rationalization mechanism that anchors model predictions in verifiable physical evidence. By extracting and analyzing pitch trajectories, energy profiles, and spectral centroids, the system effectively mitigates text-biased hallucinations and provides high-fidelity, auditable reasoning traces.\nReinforcement Learning Alignment (GRPO): Implemented Group Relative Policy Optimization (GRPO) to refine the agentâ€™s multi-step decision-making trajectories. This alignment strategy optimizes the policy-driven evidence acquisition process, rewarding logical rigor and penalizing non-informative tool invocations to ensure the reasoning path aligns with human-expert diagnostic standards.\nAmbiguity-Driven Reasoning: Formulated a novel approach to address the â€œConsensus Paradoxâ€ by treating annotator disagreement as a valuable supervision signal rather than noise. By modeling emotional ambiguity through a consensus-driven inquiry process, the framework significantly improves robustness and recovery of co-occurring minor emotions on benchmark datasets including MSP-Podcast and IEMOCAP.\n\nThis work explores how agentic multimodal models can move beyond pattern recognition toward verifiable, evidence-grounded reasoning under perceptual ambiguity.\n\n\n\n\n\n\n\n\nICASSP 2026 Accepted\n\nRecovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration\nFirst Author | ğŸ”— Paper\nDiscrete Audio Tokenization Â· Multi-layer Attention Fusion Â· SSL Â· Neural Codecs\n\nTL;DR: Introduces a multi-layer fusion framework to recover the significant performance loss in Speech Emotion Recognition (SER) caused by audio discretization, enabling semantic-rich discrete tokens to rival high-fidelity continuous features.\n\nKey Points:\n\nPerformance Recovery: Developed a novel framework that recovers 75% of the performance drop induced by discretization by integrating multi-layer WavLM tokens with 74-dimensional openSMILE paralinguistic features.\nArchitecture Innovation: Designed and benchmarked dual fusion strategiesâ€”Layer-First and Modality-Firstâ€”achieving an 8% gain with the Layer-First approach by prioritizing hierarchical feature extraction.\nAblation & Trade-offs: Conducted systematic studies across 24 WavLM layers and 5 codebook sizes (K=256 to 4000) to identify optimal compression-accuracy trade-offs for emotion preservation.\nNeural Codec Benchmarking: Evaluated against three major neural codecs (SpeechTokenizer, DAC, and EnCodec), demonstrating that semantic-rich SSL tokens provide a 50%+ performance advantage over reconstruction-focused models for downstream SER tasks.\n\nThis work establishes semantic-rich discrete speech tokens as a viable alternative to continuous acoustic features for affective computing, bridging representation learning and downstream emotion reasoning under aggressive compression constraints.\n\n\n\n\n\n\n\n\n\n\n\nNeurIPS 2025 Workshop Accepted\n\nSystematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications\nFirst Author | ğŸ”— Paper\nAI Companionship Â· Embodied Intelligence Â· Technical Taxonomy\n\nTL;DR: Systematizes the fragmented landscape of AI persona design by introducing a novel four-quadrant technical taxonomy that maps companion applications from virtual agents to embodied systems, providing actionable design guidelines for researchers and practitioners.\n\nKey Points:\n\nFour-Quadrant Taxonomy: Proposed a systematic framework categorizing AI companions along two orthogonal axesâ€”embodiment level and interaction modalityâ€”to clarify design trade-offs across diverse application domains.\nTechnical Design Patterns: Identified and documented recurring architectural patterns for persona consistency, memory management, and emotional coherence across 50+ commercial and research systems.\nEmbodiment Spectrum Analysis: Analyzed the progression from text-only virtual companions to fully embodied robotic agents, mapping key technical challenges at each stage of the embodiment continuum.\nActionable Guidelines: Synthesized practical design recommendations for balancing persona authenticity, user safety, and system scalability in real-world AI companion deployments.\n\nThis work provides a unifying systems-level lens for LLM persona agents, enabling principled design and evaluation of AI companions across virtual, multimodal, and embodied interaction settings.\n\n\n\n\n\n\n\n\n\n\n\nIEEE TPAMI 2024 Under Review\n\nForgeryGPT: Multimodal Large Language Model for Explainable Image Forgery Detection and Localization\nCo-author | ğŸ”— Paper\nFine-grained forgery localization Â· Visionâ€“language reasoning Â· Multimodal LLM grounding\n\nTL;DR: Leverages Multimodal LLMs to pioneer an explainable image forgery detection and localization system that generates natural language rationales for identified visual manipulations.\n\nKey Points:\n\nHybrid Vision Architecture: Developed a â€œVocabulary-enhanced Vision Encoderâ€ integrating a trainable ViT with a frozen CLIP encoder to capture fine-grained, domain-specific forgery artifacts.\nAdaptive Forgery Prompting: Proposed an â€œObject-agnostic Forgery Promptâ€ mechanism using 12-dimensional learnable embeddings to enable dynamic adaptation across diverse forgery scenarios, achieving a 15% improvement over baselines.\nLarge-Scale Synthesis Pipeline: Constructed a comprehensive 768K+ multimodal dataset using a multi-granularity mask generation pipeline that combines random segmentation with Segment Anything Model (SAM) techniques.\nExplainable Localization: Contributed the FL-Expert module to provide precise spatial localization coupled with linguistic explanations of splicing, copy-move, and removal manipulations.\n\n\n\n\n\n\n\n\n\n\n\n\nACM MM 2023 Accepted\n\nECENet: Explainable and Context-Enhanced Network for Multimodal Fact Verification\nCo-author | ğŸ”— Paper\nDual-granularity Attention Â· Cross-modal Alignment Â· Hierarchical Reasoning\n\nTL;DR: Introduces a state-of-the-art multimodal fact verification framework that utilizes dual-granularity attention and hierarchical reasoning to generate evidence-based justifications for news veracity.\n\nKey Points:\n\nDual-Granularity Attention: Engineered an Improved Coarse- and Fine-grained Attention Network (CFgAN) that enhanced contextual comprehension of image-text correlations by 12.1%.\nHierarchical Reasoning Framework: Developed a unified architecture for feature extraction and cross-modal fusion, enabling the system to perform complex evidence-based inference.\nBenchmark Leadership: Achieved SOTA performance on major benchmarks, including 87.7% accuracy on NewsCLIPpings and an 81.5 F1-score on FACTIFY.\nJustification Generation: Focused on â€œexplainabilityâ€ by ensuring the network provides logical justifications alongside its verification results, as showcased in oral and poster presentations at ACM MM 2023.\n\n\n\n\n\n\n\n\n\nAAAI 2023 Accepted\n\nUnimodal Feature-Enhanced and Cross-Modal Correlation Learning for Multimodal Fact Verification\nCo-author | ğŸ”— Paper\nMultimodal feature engineering Â· Cross-modal correlation learning"
  },
  {
    "objectID": "artwork.html#dinosaur",
    "href": "artwork.html#dinosaur",
    "title": "Artwork",
    "section": "Dinosaur",
    "text": "Dinosaur"
  },
  {
    "objectID": "artwork.html#random",
    "href": "artwork.html#random",
    "title": "Artwork",
    "section": "Random",
    "text": "Random"
  }
]